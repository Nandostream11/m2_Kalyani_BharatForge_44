{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5nbFjQj3SfI",
        "outputId": "7f0e1573-ed7f-4e96-f8b4-74b6e1dbe26d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Total Reward: -22.60\n",
            "Episode 50, Total Reward: -2.90\n",
            "Episode 100, Total Reward: -1.70\n",
            "Episode 150, Total Reward: -0.10\n",
            "Episode 200, Total Reward: -0.60\n",
            "Episode 250, Total Reward: 0.30\n",
            "Episode 300, Total Reward: 0.30\n",
            "Episode 350, Total Reward: 0.10\n",
            "Episode 400, Total Reward: 0.00\n",
            "Episode 450, Total Reward: 0.30\n",
            "[[ 1  0  0  0  0]\n",
            " [ 0 -1  0  0  0]\n",
            " [ 0  0 -1  0  0]\n",
            " [ 0 -1  0 -1  0]\n",
            " [ 0  0  0  0  9]]\n",
            "[[ 0  0  0  0  0]\n",
            " [ 1 -1  0  0  0]\n",
            " [ 0  0 -1  0  0]\n",
            " [ 0 -1  0 -1  0]\n",
            " [ 0  0  0  0  9]]\n",
            "[[ 0  0  0  0  0]\n",
            " [ 0 -1  0  0  0]\n",
            " [ 1  0 -1  0  0]\n",
            " [ 0 -1  0 -1  0]\n",
            " [ 0  0  0  0  9]]\n",
            "[[ 0  0  0  0  0]\n",
            " [ 0 -1  0  0  0]\n",
            " [ 0  0 -1  0  0]\n",
            " [ 1 -1  0 -1  0]\n",
            " [ 0  0  0  0  9]]\n",
            "[[ 0  0  0  0  0]\n",
            " [ 0 -1  0  0  0]\n",
            " [ 0  0 -1  0  0]\n",
            " [ 0 -1  0 -1  0]\n",
            " [ 1  0  0  0  9]]\n",
            "[[ 0  0  0  0  0]\n",
            " [ 0 -1  0  0  0]\n",
            " [ 0  0 -1  0  0]\n",
            " [ 0 -1  0 -1  0]\n",
            " [ 0  1  0  0  9]]\n",
            "[[ 0  0  0  0  0]\n",
            " [ 0 -1  0  0  0]\n",
            " [ 0  0 -1  0  0]\n",
            " [ 0 -1  0 -1  0]\n",
            " [ 0  0  1  0  9]]\n",
            "[[ 0  0  0  0  0]\n",
            " [ 0 -1  0  0  0]\n",
            " [ 0  0 -1  0  0]\n",
            " [ 0 -1  0 -1  0]\n",
            " [ 0  0  0  1  9]]\n",
            "[[ 0  0  0  0  0]\n",
            " [ 0 -1  0  0  0]\n",
            " [ 0  0 -1  0  0]\n",
            " [ 0 -1  0 -1  0]\n",
            " [ 0  0  0  0  9]]\n",
            "Target reached!\n"
          ]
        }
      ],
      "source": [
        "#Q-Learning on single bot with static obstacles\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Define the environment\n",
        "class GridWorld:\n",
        "    def __init__(self, grid_size, target_pos, obstacles):\n",
        "        self.grid_size = grid_size\n",
        "        self.target_pos = target_pos\n",
        "        self.obstacles = obstacles\n",
        "        self.state = (0, 0)  # Initial position of the robot\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = (0, 0)\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        x, y = self.state\n",
        "        new_x, new_y = x, y\n",
        "\n",
        "        # Move based on the action\n",
        "        if action == 0 and x > 0:       # Move up\n",
        "            new_x -= 1\n",
        "        elif action == 1 and x < self.grid_size - 1:  # Move down\n",
        "            new_x += 1\n",
        "        elif action == 2 and y > 0:    # Move left\n",
        "            new_y -= 1\n",
        "        elif action == 3 and y < self.grid_size - 1:  # Move right\n",
        "            new_y += 1\n",
        "\n",
        "        # Check for obstacles\n",
        "        if (new_x, new_y) not in self.obstacles:\n",
        "            self.state = (new_x, new_y)\n",
        "\n",
        "        # Calculate reward\n",
        "        reward = 1 if self.state == self.target_pos else -0.1\n",
        "        done = self.state == self.target_pos\n",
        "        return self.state, reward, done\n",
        "\n",
        "    def render(self):\n",
        "        grid = np.zeros((self.grid_size, self.grid_size), dtype=int)\n",
        "        x, y = self.state\n",
        "        grid[x, y] = 1  # Robot position\n",
        "        tx, ty = self.target_pos\n",
        "        grid[tx, ty] = 9  # Target position\n",
        "\n",
        "        for ox, oy in self.obstacles:\n",
        "            grid[ox, oy] = -1  # Obstacles\n",
        "\n",
        "        print(grid)\n",
        "\n",
        "# Q-Learning agent\n",
        "class QLearningAgent:\n",
        "    def __init__(self, grid_size, learning_rate=0.1, discount_factor=0.99, epsilon=1.0, epsilon_decay=0.995):\n",
        "        self.q_table = np.zeros((grid_size, grid_size, 4))  # Q-table with 4 actions\n",
        "        self.lr = learning_rate\n",
        "        self.gamma = discount_factor\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.actions = [0, 1, 2, 3]  # [Up, Down, Left, Right]\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return random.choice(self.actions)  # Explore\n",
        "        x, y = state\n",
        "        return np.argmax(self.q_table[x, y])  # Exploit\n",
        "\n",
        "    def update_q_value(self, state, action, reward, next_state):\n",
        "        x, y = state\n",
        "        nx, ny = next_state\n",
        "        old_value = self.q_table[x, y, action]\n",
        "        next_max = np.max(self.q_table[nx, ny])\n",
        "        new_value = old_value + self.lr * (reward + self.gamma * next_max - old_value)\n",
        "        self.q_table[x, y, action] = new_value\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(0.1, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "# Train the agent\n",
        "def train_agent(grid_size=5, target_pos=(4, 4), obstacles=[(2, 2), (3, 3)], episodes=500):\n",
        "    env = GridWorld(grid_size, target_pos, obstacles)\n",
        "    agent = QLearningAgent(grid_size)\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "            agent.update_q_value(state, action, reward, next_state)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        agent.decay_epsilon()\n",
        "\n",
        "        if episode % 50 == 0:\n",
        "            print(f\"Episode {episode}, Total Reward: {total_reward:.2f}\")\n",
        "\n",
        "    return env, agent\n",
        "\n",
        "# Test the trained agent\n",
        "def test_agent(env, agent):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    steps = 0\n",
        "\n",
        "    while not done and steps < 50:  # Limit steps to prevent infinite loops\n",
        "        env.render()\n",
        "        action = np.argmax(agent.q_table[state[0], state[1]])\n",
        "        state, _, done = env.step(action)\n",
        "        steps += 1\n",
        "\n",
        "    env.render()\n",
        "    if done:\n",
        "        print(\"Target reached!\")\n",
        "    else:\n",
        "        print(\"Failed to reach the target.\")\n",
        "\n",
        "# Main execution\n",
        "obstacles = [(1, 1), (2, 2), (3, 1), (3, 3)]  # List of obstacle positions\n",
        "env, agent = train_agent(grid_size=5, target_pos=(4, 4), obstacles=obstacles, episodes=500)\n",
        "test_agent(env, agent)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q-Learning on Single bot with dynamic obstacles\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Environment: GridWorld with Dynamic Obstacles\n",
        "class GridWorldDynamicObstacles:\n",
        "    def __init__(self, grid_size, target_pos, num_dynamic_obstacles):\n",
        "        self.grid_size = grid_size\n",
        "        self.target_pos = target_pos\n",
        "        self.num_dynamic_obstacles = num_dynamic_obstacles\n",
        "        self.state = (0, 0)  # Initial robot position\n",
        "        self.dynamic_obstacles = self._generate_dynamic_obstacles()\n",
        "\n",
        "    def _generate_dynamic_obstacles(self):\n",
        "        obstacles = []\n",
        "        for _ in range(self.num_dynamic_obstacles):\n",
        "            # Randomly place dynamic obstacles in the grid, avoiding target and robot start\n",
        "            while True:\n",
        "                obstacle = (random.randint(0, self.grid_size-1), random.randint(0, self.grid_size-1))\n",
        "                if obstacle != self.target_pos and obstacle != self.state and obstacle not in obstacles:\n",
        "                    obstacles.append(obstacle)\n",
        "                    break\n",
        "        return obstacles\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = (0, 0)\n",
        "        self.dynamic_obstacles = self._generate_dynamic_obstacles()\n",
        "        return self.state\n",
        "\n",
        "    def move_dynamic_obstacles(self):\n",
        "        # Move dynamic obstacles randomly within the grid\n",
        "        new_obstacles = []\n",
        "        for ox, oy in self.dynamic_obstacles:\n",
        "            move_dir = random.choice([(1, 0), (-1, 0), (0, 1), (0, -1)])  # Down, Up, Right, Left\n",
        "            new_ox = min(max(ox + move_dir[0], 0), self.grid_size - 1)\n",
        "            new_oy = min(max(oy + move_dir[1], 0), self.grid_size - 1)\n",
        "            if (new_ox, new_oy) != self.target_pos and (new_ox, new_oy) != self.state and (new_ox, new_oy) not in new_obstacles:\n",
        "                new_obstacles.append((new_ox, new_oy))\n",
        "            else:\n",
        "                new_obstacles.append((ox, oy))  # Keep the obstacle in place if moved invalidly\n",
        "        self.dynamic_obstacles = new_obstacles\n",
        "\n",
        "    def step(self, action):\n",
        "        x, y = self.state\n",
        "        new_x, new_y = x, y\n",
        "\n",
        "        # Move based on the action\n",
        "        if action == 0 and x > 0:       # Move up\n",
        "            new_x -= 1\n",
        "        elif action == 1 and x < self.grid_size - 1:  # Move down\n",
        "            new_x += 1\n",
        "        elif action == 2 and y > 0:    # Move left\n",
        "            new_y -= 1\n",
        "        elif action == 3 and y < self.grid_size - 1:  # Move right\n",
        "            new_y += 1\n",
        "\n",
        "        # Check for collisions with dynamic obstacles\n",
        "        if (new_x, new_y) not in self.dynamic_obstacles:\n",
        "            self.state = (new_x, new_y)\n",
        "\n",
        "        # Calculate reward\n",
        "        reward = 1 if self.state == self.target_pos else -0.1\n",
        "        done = self.state == self.target_pos\n",
        "        return self.state, reward, done\n",
        "\n",
        "    def render(self):\n",
        "        grid = np.zeros((self.grid_size, self.grid_size), dtype=int)\n",
        "        x, y = self.state\n",
        "        grid[x, y] = 1  # Robot position\n",
        "        tx, ty = self.target_pos\n",
        "        grid[tx, ty] = 9  # Target position\n",
        "\n",
        "        for ox, oy in self.dynamic_obstacles:\n",
        "            grid[ox, oy] = -1  # Dynamic obstacles\n",
        "\n",
        "        print(grid)\n",
        "\n",
        "\n",
        "# QLearning Agent\n",
        "class QLearningAgent:\n",
        "    def __init__(self, grid_size, learning_rate=0.1, discount_factor=0.99, epsilon=1.0, epsilon_decay=0.995):\n",
        "        self.q_table = np.zeros((grid_size, grid_size, 4))  # Q-table with 4 actions (Up, Down, Left, Right)\n",
        "        self.lr = learning_rate\n",
        "        self.gamma = discount_factor\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.actions = [0, 1, 2, 3]  # Actions: Up, Down, Left, Right\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return random.choice(self.actions)  # Explore\n",
        "        x, y = state\n",
        "        return np.argmax(self.q_table[x, y])  # Exploit (choose best action)\n",
        "\n",
        "    def update_q_value(self, state, action, reward, next_state):\n",
        "        x, y = state\n",
        "        nx, ny = next_state\n",
        "        old_value = self.q_table[x, y, action]\n",
        "        next_max = np.max(self.q_table[nx, ny])\n",
        "        new_value = old_value + self.lr * (reward + self.gamma * next_max - old_value)\n",
        "        self.q_table[x, y, action] = new_value\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon *= self.epsilon_decay  # Decay epsilon for exploration-exploitation trade-off\n",
        "\n",
        "\n",
        "# Training the Agent\n",
        "def train_agent():\n",
        "    grid_size = 5\n",
        "    target_pos = (4, 4)  # Target position\n",
        "    num_dynamic_obstacles = 2  # Number of dynamic obstacles\n",
        "\n",
        "    env = GridWorldDynamicObstacles(grid_size, target_pos, num_dynamic_obstacles)\n",
        "    agent = QLearningAgent(grid_size)\n",
        "\n",
        "    num_episodes = 1000\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "            agent.update_q_value(state, action, reward, next_state)\n",
        "            state = next_state\n",
        "\n",
        "            # Move dynamic obstacles every step\n",
        "            env.move_dynamic_obstacles()\n",
        "\n",
        "            total_reward += reward\n",
        "\n",
        "        agent.decay_epsilon()\n",
        "\n",
        "        # Print progress for every 100 episodes\n",
        "        if episode % 100 == 0:\n",
        "            print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
        "\n",
        "    return agent, env\n",
        "\n",
        "\n",
        "# Running the trained agent in the environment\n",
        "def run_agent(agent, env):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = agent.choose_action(state)\n",
        "        next_state, reward, done = env.step(action)\n",
        "        state = next_state\n",
        "        env.render()\n",
        "        print(f\"Action taken: {action}, Reward: {reward}\")\n",
        "        env.move_dynamic_obstacles()  # Move dynamic obstacles in each step\n",
        "\n",
        "# Training\n",
        "agent, env = train_agent()\n",
        "\n",
        "# Testing the trained agent\n",
        "run_agent(agent, env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FSTn49U3mzM",
        "outputId": "fac5224f-b28e-4041-8d98-501a9ec419d9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Total Reward: -5.999999999999991\n",
            "Episode 100, Total Reward: 1.1102230246251565e-16\n",
            "Episode 200, Total Reward: -0.19999999999999996\n",
            "Episode 300, Total Reward: 0.30000000000000004\n",
            "Episode 400, Total Reward: 0.20000000000000007\n",
            "Episode 500, Total Reward: 0.20000000000000007\n",
            "Episode 600, Total Reward: 0.30000000000000004\n",
            "Episode 700, Total Reward: -0.09999999999999987\n",
            "Episode 800, Total Reward: 0.30000000000000004\n",
            "Episode 900, Total Reward: 0.30000000000000004\n",
            "[[ 0  0  0  0  0]\n",
            " [ 1  0 -1  0  0]\n",
            " [ 0  0  0  0  0]\n",
            " [ 0  0  0  0  0]\n",
            " [-1  0  0  0  9]]\n",
            "Action taken: 1, Reward: -0.1\n",
            "[[ 0  0 -1  0  0]\n",
            " [ 0  0  0  0  0]\n",
            " [ 1  0  0  0  0]\n",
            " [ 0  0  0  0  0]\n",
            " [ 0 -1  0  0  9]]\n",
            "Action taken: 1, Reward: -0.1\n",
            "[[ 0 -1  0  0  0]\n",
            " [ 0  0  0  0  0]\n",
            " [ 0  1  0  0  0]\n",
            " [ 0  0  0  0  0]\n",
            " [ 0  0 -1  0  9]]\n",
            "Action taken: 3, Reward: -0.1\n",
            "[[ 0  0 -1  0  0]\n",
            " [ 0  0  0  0  0]\n",
            " [ 0  0  1  0  0]\n",
            " [ 0  0  0  0  0]\n",
            " [ 0 -1  0  0  9]]\n",
            "Action taken: 3, Reward: -0.1\n",
            "[[ 0 -1  0  0  0]\n",
            " [ 0  0  0  0  0]\n",
            " [ 0  0  0  1  0]\n",
            " [ 0  0  0  0  0]\n",
            " [-1  0  0  0  9]]\n",
            "Action taken: 3, Reward: -0.1\n",
            "[[-1  0  0  0  0]\n",
            " [ 0  0  0  0  0]\n",
            " [ 0  0  0  0  0]\n",
            " [ 0  0  0  1  0]\n",
            " [-1  0  0  0  9]]\n",
            "Action taken: 1, Reward: -0.1\n",
            "[[ 0  0  0  0  0]\n",
            " [-1  0  0  0  0]\n",
            " [ 0  0  0  0  0]\n",
            " [ 0  0  0  0  1]\n",
            " [-1  0  0  0  9]]\n",
            "Action taken: 3, Reward: -0.1\n",
            "[[ 0  0  0  0  0]\n",
            " [ 0 -1  0  0  0]\n",
            " [ 0  0  0  0  0]\n",
            " [ 0  0  0  0  0]\n",
            " [-1  0  0  0  9]]\n",
            "Action taken: 1, Reward: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9OAbE0HBFOqG"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}